{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from medpy import metric\n",
    "from collections import Counter\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Set the device: cpu or gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")\n",
    "\n",
    "if is_cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f'Preparing to use device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/pabharathi/Documents/GOES/Data/Data/train_data_scaled.pkl','rb') as f:\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saved shape:\", x.shape)\n",
    "x = x.transpose(0,3,1,2)\n",
    "print(\"Reshaped to:\", x.shape)\n",
    "num_channels = len(x [0,:, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/pabharathi/Documents/GOES/Data/Data/train_counts.pkl','rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the channel-wise mean for the images that are classified as zero\n",
    "#Create a baseline zero tensor using the channel-wise averages\n",
    "#Later used as a potential baseline for Integrated Gradients and Deeplift\n",
    "tensor = torch.tensor((), dtype=torch.float)\n",
    "base_zero= torch.zeros([num_channels, 32, 32])\n",
    "zero_idx = np.argwhere(y.squeeze() == 0).squeeze()\n",
    "for i in range(0, num_channels):\n",
    "    currchannel_mean = np.mean(x[zero_idx, i, :, :].flatten())\n",
    "    base_zero[i] = tensor.new_full((32, 32), currchannel_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saved shape:\", y.shape)\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "print(\"Reshaped to:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt0 = list(np.where(y >= 1)[0])\n",
    "# lt0 = list(np.where(y < 1)[0])\n",
    "# take_a_sample = random.sample(lt0, 10)\n",
    "\n",
    "# sample = gt0 + take_a_sample\n",
    "\n",
    "# x = x[sample]\n",
    "# y = y[sample]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train / test partitions (80 / 20 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state = 5000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a histogram of the (log) counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(Y_train),bins=range(200))\n",
    "plt.xlim([0,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create labels for binned lightning counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [10, 100]\n",
    "\n",
    "y_train = np.where(Y_train[:] > 0.0, 1, 0)\n",
    "y_test = np.where(Y_test[:] > 0.0, 1, 0)\n",
    "\n",
    "for p,q in enumerate(bins):\n",
    "    y_train = np.where(Y_train[:] >= q, p+2, y_train)\n",
    "    y_test = np.where(Y_test[:] >= q, p+2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "X_IG = X_test\n",
    "Y_IG = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample thirty of each of the classes for use in Integrated Gradients\n",
    "#First get all the indices for each class then sample twenty images from each\n",
    "#Comment out classes that do not exist in the current iteration\n",
    "rng = np.random.default_rng(2021) #Set the seed for rng\n",
    "test_classsize = 30\n",
    "\n",
    "zero_idx = np.argwhere(y_test.squeeze() == 0).squeeze()\n",
    "zero_randomidx = rng.choice(zero_idx, size=test_classsize, replace=False )\n",
    "\n",
    "one_idx = np.argwhere(y_test.squeeze() == 1).squeeze()\n",
    "one_randomidx = rng.choice(one_idx, size=test_classsize, replace=False )\n",
    "\n",
    "two_idx = np.argwhere(y_test.squeeze() == 2).squeeze()\n",
    "two_randomidx = rng.choice(two_idx, size=test_classsize, replace=False )\n",
    "\n",
    "three_idx = np.argwhere(y_test.squeeze() == 3).squeeze()\n",
    "three_randomidx = rng.choice(three_idx, size=test_classsize, replace=False )\n",
    "\n",
    "test_randomidx = np.concatenate((zero_randomidx, one_randomidx, two_randomidx, three_randomidx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class weights based on counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for val in y_train:\n",
    "    counts[val[0]] += 1\n",
    "counts = dict(counts)\n",
    "\n",
    "#weights = [1 - (counts[x] / sum(counts.values())) for x in sorted(counts.keys())]\n",
    "weights = [np.log1p(max(counts.values()) / counts[x]) for x in sorted(counts.keys())]\n",
    "weights = [x / max(weights) for x in weights]\n",
    "weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original Resnet model had to be modified to suit Captum's Requirement for Deeplift\n",
    "# To overcome this error, an implementation of Resnet18 from the torchvision github was\n",
    "# copied over and the modifying the BasicBlock module. The BasicBlock was modified so that\n",
    "# the relu activation layers are redefined each time instead of being reused for subsequent uses.\n",
    "# The performance of this modified Resnet model is comparable to the orignical implementation\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=4, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv1d(num_channels, 64, (7, 7), (2, 2), (3, 3), bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "    \n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "def _resnet(arch, block, layers, num_classes, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, num_classes =num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url('https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, progress=False, num_classes = 4, **kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], num_classes, pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        #Add another relu layer\n",
    "        \n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "\n",
    "        # Modified to use relu2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, fcl_layers = [], dr = 0.0, output_size = 1, resnet_model = 18, pretrained = True):\n",
    "#         super(ResNet, self).__init__()\n",
    "#         self.pretrained = pretrained\n",
    "#         self.resnet_model = resnet_model \n",
    "#         if self.resnet_model == 18:\n",
    "#             resnet = models.resnet18(pretrained=self.pretrained)\n",
    "#         elif self.resnet_model == 34:\n",
    "#             resnet = models.resnet34(pretrained=self.pretrained)\n",
    "#         elif self.resnet_model == 50:\n",
    "#             resnet = models.resnet50(pretrained=self.pretrained)\n",
    "#         elif self.resnet_model == 101:\n",
    "#             resnet = models.resnet101(pretrained=self.pretrained)\n",
    "#         elif self.resnet_model == 152:\n",
    "#             resnet = models.resnet152(pretrained=self.pretrained)\n",
    "#         resnet.conv1 = torch.nn.Conv1d(4, 64, (7, 7), (2, 2), (3, 3), bias=False)\n",
    "#         modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "#         self.resnet_output_dim = resnet.fc.in_features\n",
    "#         self.resnet = nn.Sequential(*modules)\n",
    "#         self.fcn = self.make_fcn(self.resnet_output_dim, output_size, fcl_layers, dr)\n",
    "        \n",
    "#     def make_fcn(self, input_size, output_size, fcl_layers, dr):\n",
    "#         if len(fcl_layers) > 0:\n",
    "#             fcn = [\n",
    "# #                 nn.Dropout(dr),\n",
    "#                 nn.Linear(input_size, fcl_layers[0]),\n",
    "# #                 nn.BatchNorm1d(fcl_layers[0]),\n",
    "#                 torch.nn.ReLU()\n",
    "#             ]\n",
    "#             if len(fcl_layers) == 1:\n",
    "#                 fcn.append(nn.Linear(fcl_layers[0], output_size))\n",
    "#             else:\n",
    "#                 for i in range(len(fcl_layers)-1):\n",
    "#                     fcn += [\n",
    "#                         nn.Linear(fcl_layers[i], fcl_layers[i+1]),\n",
    "# #                         nn.BatchNorm1d(fcl_layers[i+1]),\n",
    "#                         torch.nn.ReLU(),\n",
    "# #                         nn.Dropout(dr)\n",
    "#                     ]\n",
    "#                 fcn.append(nn.Linear(fcl_layers[i+1], output_size))\n",
    "#         else:\n",
    "#             fcn = [\n",
    "# #                 nn.Dropout(dr),\n",
    "#                 nn.Linear(input_size, output_size)\n",
    "#             ]\n",
    "#         if output_size > 1:\n",
    "#             fcn.append(torch.nn.LogSoftmax(dim=1))\n",
    "#         return nn.Sequential(*fcn)\n",
    "\n",
    "#     def _make_layer(self, block, planes, num_blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(self.in_planes, planes, stride))\n",
    "#             self.in_planes = planes * block.expansion\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.resnet(x)\n",
    "#         x = x.view(x.size(0), -1)  # flatten\n",
    "#         x = self.fcn(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = len(weights) \n",
    "fcl_layers = []\n",
    "dropout = 0.5\n",
    "\n",
    "model = resnet18(num_classes = output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model to make sure the architecture is consistent on a batch size of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_train[:2]).float().to(device)\n",
    "print(X.shape)\n",
    "g = model(X).exp()\n",
    "#print(torch.max(g,1)) # exp to turn the logits into probabilities, since we used LogSoftmax\n",
    "print(g)\n",
    "#print(torch.argmax(g,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-03\n",
    "weight_decay = 1e-09\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Custom Loss Function\n",
    "The modified loss function comes with label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def k_one_hot(self, targets:torch.Tensor, n_classes:int, smoothing=0.0):\n",
    "        with torch.no_grad():\n",
    "            targets = torch.empty(size=(targets.size(0), n_classes),\n",
    "                                  device=targets.device) \\\n",
    "                                  .fill_(smoothing /(n_classes-1)) \\\n",
    "                                  .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n",
    "        return targets\n",
    "\n",
    "    def reduce_loss(self, loss):\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n",
    "        if self.reduction == 'sum' else loss\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "\n",
    "        targets = self.k_one_hot(targets, inputs.size(-1), self.smoothing)\n",
    "        log_preds = F.log_softmax(inputs, -1)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            log_preds = log_preds * self.weight.unsqueeze(0)\n",
    "\n",
    "        return self.reduce_loss(-(targets * log_preds).sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_criterion = SmoothCrossEntropyLoss(weight = weights) #weight = weights, smoothing = 0.1) \n",
    "test_criterion = torch.nn.CrossEntropyLoss()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Top-K Accuracy Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the k top predictions for the specified values of k\n",
    "    In top-5 accuracy you give yourself credit for having the right answer\n",
    "    if the right answer appears in your top five guesses.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # ---- get the topk most likely labels according to your model\n",
    "        # get the largest k \\in [n_classes] (i.e. the number of most likely probabilities we will use)\n",
    "        maxk = max(topk)  # max number labels we will consider in the right choices for out model\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        # get top maxk indicies that correspond to the most likely probability scores\n",
    "        # (note _ means we don't care about the actual top maxk scores just their corresponding indicies/labels)\n",
    "        _, y_pred = output.topk(k=maxk, dim=1)  # _, [B, n_classes] -> [B, maxk]\n",
    "        y_pred = y_pred.t()  # [B, maxk] -> [maxk, B] Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n",
    "\n",
    "        # - get the credit for each example if the models predictions is in maxk values (main crux of code)\n",
    "        # for any example, the model will get credit if it's prediction matches the ground truth\n",
    "        # for each example we compare if the model's best prediction matches the truth. If yes we get an entry of 1.\n",
    "        # if the k'th top answer of the model matches the truth we get 1.\n",
    "        # Note: this for any example in batch we can only ever get 1 match (so we never overestimate accuracy <1)\n",
    "        target_reshaped = target.view(1, -1).expand_as(y_pred)  # [B] -> [B, 1] -> [maxk, B]\n",
    "        # compare every topk's model prediction with the ground truth & give credit if any matches the ground truth\n",
    "        correct = (y_pred == target_reshaped)  # [maxk, B] were for each example we know which topk prediction matched truth\n",
    "        # original: correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        # -- get topk accuracy\n",
    "        list_topk_accs = []  # idx is topk1, topk2, ... etc\n",
    "        for k in topk:\n",
    "            # get tensor of which topk answer was right\n",
    "            ind_which_topk_matched_truth = correct[:k]  # [maxk, B] -> [k, B]\n",
    "            # flatten it to help compute if we got it correct for each example in batch\n",
    "            flattened_indicator_which_topk_matched_truth = ind_which_topk_matched_truth.reshape(-1).float()  # [k, B] -> [kB]\n",
    "            # get if we got it right for any of our top k prediction for each example in batch\n",
    "            tot_correct_topk = flattened_indicator_which_topk_matched_truth.float().sum(dim=0, keepdim=True)  # [kB] -> [1]\n",
    "            # compute topk accuracy - the accuracy of the mode's ability to get it right within it's top k guesses/preds\n",
    "            topk_acc = tot_correct_topk / batch_size  # topk accuracy for entire batch\n",
    "            list_topk_accs.append(topk_acc.item())\n",
    "        return list_topk_accs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "train_batch_size = 5\n",
    "valid_batch_size = 128\n",
    "batches_per_epoch = 500\n",
    "\n",
    "topk = (1, 2)\n",
    "patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will help lower the learning rate when the model stops improving\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    patience = 10, \n",
    "    min_lr = 1.0e-10,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Iterators for Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),\n",
    "    batch_size=train_batch_size, \n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(X_test), torch.from_numpy(y_test)),\n",
    "    batch_size=valid_batch_size,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ### Train the model \n",
    "    model.train()\n",
    "\n",
    "    # Shuffle the data first\n",
    "    batch_loss = []\n",
    "    accuracy = {k: [] for k in topk}\n",
    "    indices = list(range(X_train.shape[0]))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # Now split into batches\n",
    "    train_batches_per_epoch = int(X_train.shape[0] / train_batch_size) \n",
    "    train_batches_per_epoch = min(batches_per_epoch, train_batches_per_epoch)\n",
    "    \n",
    "    # custom tqdm so we can see the progress\n",
    "    batch_group_generator = tqdm.tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=train_batches_per_epoch, \n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for k, (x, y) in batch_group_generator:\n",
    "\n",
    "        # Converting to torch tensors and moving to GPU\n",
    "        inputs = x.float().to(device)\n",
    "        lightning_counts = y.long().to(device)\n",
    "\n",
    "        # Clear gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        pred_lightning_counts = model(inputs)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = train_criterion(pred_lightning_counts, lightning_counts.squeeze(-1))\n",
    "        \n",
    "        # compute the top-k accuracy\n",
    "        acc = torch_accuracy(pred_lightning_counts.cpu(), lightning_counts.cpu(), topk = topk)\n",
    "        for i,l in enumerate(topk):\n",
    "            accuracy[l] += [acc[i]]\n",
    "\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # update tqdm\n",
    "        to_print = \"Epoch {} train_loss: {:.4f}\".format(epoch, np.mean(batch_loss))\n",
    "        for l in sorted(accuracy.keys()):\n",
    "            to_print += \" top-{}_acc: {:.4f}\".format(l,np.mean(accuracy[l]))\n",
    "        #to_print += \" top-2_acc: {:.4f}\".format(np.mean(accuracy[2])\n",
    "        #to_print += \" top-3_acc: {:.4f}\".format(np.mean(accuracy[3]))\n",
    "        to_print += \" lr: {:.12f}\".format(optimizer.param_groups[0]['lr'])\n",
    "        batch_group_generator.set_description(to_print)\n",
    "        batch_group_generator.update()\n",
    "                                  \n",
    "        if k >= train_batches_per_epoch and k > 0:\n",
    "            break\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ### Test the model \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_loss = []\n",
    "        accuracy = {k: [] for k in topk}\n",
    "        \n",
    "        # custom tqdm so we can see the progress\n",
    "        valid_batches_per_epoch = int(X_test.shape[0] / valid_batch_size) \n",
    "        batch_group_generator = tqdm.tqdm(\n",
    "            test_loader, \n",
    "            total=valid_batches_per_epoch, \n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        for (x, y) in batch_group_generator:\n",
    "            # Converting to torch tensors and moving to GPU\n",
    "            inputs = x.float().to(device)\n",
    "            lightning_counts = y.long().to(device)\n",
    "            # get output from the model, given the inputs\n",
    "            pred_lightning_counts = model(inputs)\n",
    "            # get loss for the predicted output\n",
    "            loss = test_criterion(pred_lightning_counts, lightning_counts.squeeze(-1))\n",
    "            batch_loss.append(loss.item())\n",
    "            # compute the accuracy\n",
    "            acc = torch_accuracy(pred_lightning_counts, lightning_counts, topk = topk)\n",
    "            for i,k in enumerate(topk):\n",
    "                accuracy[k] += [acc[i]]\n",
    "            # update tqdm\n",
    "            to_print = \"Epoch {} test_loss: {:.4f}\".format(epoch, np.mean(batch_loss))\n",
    "            for k in sorted(accuracy.keys()):\n",
    "                to_print += \" top-{}_acc: {:.4f}\".format(k,np.mean(accuracy[k]))\n",
    "            batch_group_generator.set_description(to_print)\n",
    "            batch_group_generator.update()\n",
    "\n",
    "    test_loss = 1 - np.mean(accuracy[1])\n",
    "    epoch_test_losses.append(test_loss)\n",
    "    \n",
    "    # Lower the learning rate if we are not improving\n",
    "    lr_scheduler.step(test_loss)\n",
    "\n",
    "    # Save the model if its the best so far.\n",
    "    if test_loss == min(epoch_test_losses):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': test_loss\n",
    "        }\n",
    "        torch.save(state_dict, \"best.pt\")\n",
    "        \n",
    "    # Stop training if we have not improved after X epochs\n",
    "    best_epoch = [i for i,j in enumerate(epoch_test_losses) if j == min(epoch_test_losses)][-1]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\n",
    "    \"best.pt\",\n",
    "    map_location=lambda storage, loc: storage\n",
    ")\n",
    "best_epoch = checkpoint[\"epoch\"]\n",
    "#model = Net(filter_sizes, fcl_layers).to(device)\n",
    "model = resnet18().to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict on the test dataset with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = (1,2)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    batch_loss = []\n",
    "    accuracy = {k: [] for k in topk}\n",
    "    # split test data into batches\n",
    "\n",
    "    valid_batches_per_epoch = int(X_test.shape[0] / valid_batch_size) \n",
    "    batch_group_generator = tqdm.tqdm(\n",
    "        test_loader, \n",
    "        total=valid_batches_per_epoch, \n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    for (x, y) in batch_group_generator:\n",
    "        # Converting to torch tensors and moving to GPU\n",
    "        inputs = x.float().to(device)\n",
    "        lightning_counts = y.long().to(device)\n",
    "        # get output from the model, given the inputs\n",
    "        pred_lightning_counts = model(inputs)\n",
    "        # get loss for the predicted output\n",
    "        loss = test_criterion(pred_lightning_counts, lightning_counts.squeeze(-1))\n",
    "        batch_loss.append(loss.item())\n",
    "        # compute the accuracy\n",
    "        acc = torch_accuracy(pred_lightning_counts, lightning_counts, topk = topk)\n",
    "        for i,k in enumerate(topk):\n",
    "            accuracy[k] += [acc[i]]\n",
    "        \n",
    "        y_true.append(lightning_counts.squeeze(-1))\n",
    "        # Taking the top-1 answer here, but here is where we could compute the average predicted rather than take top-1\n",
    "        y_pred.append(torch.argmax(pred_lightning_counts, 1))\n",
    "\n",
    "y_true = torch.cat(y_true, axis = 0)\n",
    "y_pred = torch.cat(y_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true.cpu().numpy()\n",
    "y_pred = y_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"val_loss\", np.mean(batch_loss))\n",
    "for k in topk:\n",
    "    print(f\"top-{k} {np.mean(accuracy[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_true,y_pred,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy on all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in list(set(y_true)):\n",
    "    c = (y_true == label)\n",
    "    print(label, (y_true[c] == y_pred[c]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix( y_true, y_pred, normalize = 'true')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cm, index = sorted(list(set(y_true))), columns = sorted(list(set(y_true))))\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.ylabel(\"Actual lighting class\", fontsize = 12)\n",
    "plt.xlabel(\"Predicted lighting class\", fontsize = 14)\n",
    "plt.savefig('allbands_Confusion.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_true, y_pred, digits =3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.cm, matplotlib.colors\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "# import captum\n",
    "\n",
    "\n",
    "# from captum.attr import LayerLRP\n",
    "\n",
    "# from captum.attr import GradientShap\n",
    "# from captum.attr import Occlusion\n",
    "# from captum.attr import NoiseTunnel\n",
    "\n",
    "from captum.attr import DeepLift\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "from enum import Enum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of attributions in XAI mathods\n",
    "#Modified version of the one in Captum's visualization package\n",
    "\n",
    "class VisualizeSign(Enum):\n",
    "    positive = 1\n",
    "    absolute_value = 2\n",
    "    negative = 3\n",
    "    all = 4\n",
    "\n",
    "def _prepare_image(attr_visual):\n",
    "    return np.clip(attr_visual.astype(int), 0, 255)\n",
    "\n",
    "\n",
    "def _normalize_scale(attr, scale_factor):\n",
    "    assert scale_factor != 0, \"Cannot normalize by scale factor = 0\"\n",
    "    if abs(scale_factor) < 1e-5:\n",
    "        warnings.warn(\n",
    "            \"Attempting to normalize by value approximately 0, visualized results\"\n",
    "            \"may be misleading. This likely means that attribution values are all\"\n",
    "            \"close to 0.\"\n",
    "        )\n",
    "    attr_norm = attr / scale_factor\n",
    "    return np.clip(attr_norm, -1, 1)\n",
    "\n",
    "\n",
    "def _cumulative_sum_threshold(values, percentile):\n",
    "    # given values should be non-negative\n",
    "    assert percentile >= 0 and percentile <= 100, (\n",
    "        \"Percentile for thresholding must be \" \"between 0 and 100 inclusive.\"\n",
    "    )\n",
    "    sorted_vals = np.sort(values.flatten())\n",
    "    cum_sums = np.cumsum(sorted_vals)\n",
    "    threshold_id = np.where(cum_sums >= cum_sums[-1] * 0.01 * percentile)[0][0]\n",
    "    return sorted_vals[threshold_id]\n",
    "\n",
    "\n",
    "def _normalize_image_attr(\n",
    "    attr, sign: str, outlier_perc:2\n",
    "):\n",
    "    attr_combined = np.sum(attr, axis=2)\n",
    "    # Choose appropriate signed values and rescale, removing given outlier percentage.\n",
    "    if VisualizeSign[sign] == VisualizeSign.all:\n",
    "        threshold = _cumulative_sum_threshold(np.abs(attr_combined), 100 - outlier_perc)\n",
    "    elif VisualizeSign[sign] == VisualizeSign.positive:\n",
    "        attr_combined = (attr_combined > 0) * attr_combined\n",
    "        threshold = _cumulative_sum_threshold(attr_combined, 100 - outlier_perc)\n",
    "    elif VisualizeSign[sign] == VisualizeSign.negative:\n",
    "        attr_combined = (attr_combined < 0) * attr_combined\n",
    "        threshold = -1 * _cumulative_sum_threshold(\n",
    "            np.abs(attr_combined), 100 - outlier_perc\n",
    "        )\n",
    "    elif VisualizeSign[sign] == VisualizeSign.absolute_value:\n",
    "        attr_combined = np.abs(attr_combined)\n",
    "        threshold = _cumulative_sum_threshold(attr_combined, 100 - outlier_perc)\n",
    "    else:\n",
    "        raise AssertionError(\"Visualize Sign type is not valid.\")\n",
    "    return _normalize_scale(attr_combined, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array of images from the randomly chosen indices\n",
    "all_images = X_IG[test_randomidx, :, :, :]\n",
    "print(all_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the model output for all the randomly chosen inputs\n",
    "#Store the values in a dictionary for later use\n",
    "predicted_dict = defaultdict(list)\n",
    "total_outputs = []\n",
    "for idx, input1 in enumerate(all_images):\n",
    "    input1 = torch.tensor(input1)\n",
    "    input1= input1.unsqueeze(0)\n",
    "    output = model(input1)\n",
    "    output = F.softmax(output, dim =1)\n",
    "    prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "    predicted_dict[pred_label_idx.item()].append(idx)\n",
    "    total_outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Deeplift model and run it for each of the inputs\n",
    "# with respect to each of the picture's predicted class (target is set to label of predicted class)\n",
    "# Current implementation of deeplift raises warning, but expected to go away in future versions\n",
    "dl = DeepLift(model)\n",
    "attributions_mult = []\n",
    "for idx, input1 in enumerate(all_images):\n",
    "    input1 = torch.tensor(input1)\n",
    "    input1 = input1.unsqueeze(0)\n",
    "    prediction_score, pred_label_idx = torch.topk(total_outputs[idx], 1)\n",
    "    attributions_dl = dl.attribute(input1, target = pred_label_idx)\n",
    "    attributions_mult.append(np.transpose(attributions_dl.squeeze().cpu().detach().numpy(), (1,2,0)))\n",
    "attributions_mult = np.array(attributions_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure that the shape of the attributions alligns with that of the transposed images\n",
    "# We needed to tranpose because visualization packages require channels to be the last dim\n",
    "print(attributions_mult.shape, np.transpose(all_images, (0, 2, 3, 1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various plots depicting attributions against various parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a 2D histogram of the pixel values against the attributions for the attributions\n",
    "samp_all= np.transpose(all_images, (0, 2, 3, 1))\n",
    "plt.figure(figsize=(7.5,5))\n",
    "plt.xlabel(\"Pixel Value\", fontsize = 14)\n",
    "plt.ylabel(\"Deeplift Attribution\", fontsize = 14)\n",
    "plt.title(\"Density Heatmap of Inputs and Attributions\", fontsize = 16, pad =15)\n",
    "hi_ = plt.hist2d(samp_all.flatten(), attributions_mult.flatten(), range= [[0.0, 1.0], [-0.15, 0.15]], density = True, bins= (50, 50), cmap = plt.cm.jet, norm = matplotlib.colors.LogNorm())\n",
    "plt.colorbar()\n",
    "plt.savefig(\"DL_InpVsAttr.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.constrained_layout.use'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-d Histograms showing how input values as well as attributions vary acrosss true bins\n",
    "def attr_classviz(attributions, Range, predictions = None, size = 4, Share = True, Pred = False, y_label = \"Integrated Gradient Attributions\"):\n",
    "    if(size == 2):\n",
    "        m,n = 2,1\n",
    "        h,w = 9, 12\n",
    "    if(size == 3):\n",
    "        m,n = 3,1\n",
    "        h, w = 8.5,19\n",
    "    if(size == 4):\n",
    "        m,n = 2,2\n",
    "        h,w = 18, 12\n",
    "        \n",
    "    fig, axs = plt.subplots(m, n, figsize = (h, w), sharey = Share)\n",
    "    classifications = ['No Lightning', '1-10 Lightning', '10-100 Lightning', '100+ Lightning']\n",
    "\n",
    "    it = 0\n",
    "    \n",
    "    if(n == 1):\n",
    "        for j in range (m):\n",
    "            axs[j].set_title(\"Inputs and Attributions for \" +\n",
    "                            classifications[it], fontsize = 16, pad=15)\n",
    "            axs[j].set_xlabel(\"Pixel Value\", fontsize =14)\n",
    "            axs[j].set_ylabel(y_label, fontsize =14)\n",
    "            if(Pred==False):\n",
    "                hi_ = axs[j].hist2d(samp_all[test_classsize*(it):test_classsize+test_classsize*(it), :, :, :].flatten(), \n",
    "                              attributions[test_classsize *(it):test_classsize+test_classsize*(it), :, :, :].flatten(),\n",
    "                                    range= Range,\n",
    "                              density = True, bins= (50, 50), cmap = plt.cm.jet, norm = matplotlib.colors.LogNorm())\n",
    "            else:\n",
    "                hi_ = axs[j].hist2d(samp_all[predictions[it], :, :, :].flatten(), \n",
    "                              attributions[predictions[it], :, :, :].flatten(), range= Range,\n",
    "                              density = True, bins= (50, 50), cmap = plt.cm.jet, norm = matplotlib.colors.LogNorm())\n",
    "            ax = axs[j]\n",
    "            fig.colorbar(hi_[3], ax = ax)\n",
    "            it += 1\n",
    "    else:\n",
    "        \n",
    "        for i in range(m):\n",
    "            for j in range (n):\n",
    "                axs[i, j].set_title(\"Inputs and Attributions for \" +\n",
    "                            classifications[it], fontsize = 16, pad=15)\n",
    "                axs[i,j].set_xlabel(\"Pixel Value\", fontsize =14)\n",
    "                axs[i,j].set_ylabel(y_label, fontsize =14)\n",
    "                if(Pred==False):\n",
    "                    hi_ = axs[i,j].hist2d(samp_all[test_classsize*(it):test_classsize+test_classsize*(it), :, :, :].flatten(), \n",
    "                              attributions[test_classsize *(it):test_classsize+test_classsize*(it), :, :, :].flatten(),range= Range,\n",
    "                              density = True, bins= (50, 50), cmap = plt.cm.jet, norm = matplotlib.colors.LogNorm())\n",
    "                else:\n",
    "                    hi_ = axs[i,j].hist2d(samp_all[predictions[it], :, :, :].flatten(), \n",
    "                              attributions[predictions[it], :, :, :].flatten(), range= Range,\n",
    "                              density = True, bins= (50, 50), cmap = plt.cm.jet, norm = matplotlib.colors.LogNorm())\n",
    "                ax = axs[i,j]\n",
    "                fig.colorbar(hi_[3], ax = ax)\n",
    "                it += 1\n",
    "\n",
    "    plt.subplots_adjust(left=0.2,\n",
    "                    bottom=0.2, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.2, \n",
    "                    hspace=0.3)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = attr_classviz(attributions = attributions_mult, Range= [[0.0, 1.0], [-0.15, 0.15]], Share= False, y_label = \"Deeplift Attributions\" )\n",
    "_.savefig(\"DLact_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Input vs attributions for each predicted class. Note this is usally less than the classifications in the model\n",
    "_ = attr_classviz(attributions = attributions_mult, predictions = predicted_dict,  Range= [[0.0, 1.0], [-0.15, 0.15]], Share= False, size = len(predicted_dict), Pred=True, y_label = \"Deeplift Attributions\")\n",
    "_.savefig(\"DLpred_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of standard deeplif from a bin 3 image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#000000'),\n",
    "                                                  (1, '#000000')], N=256)\n",
    "\n",
    "_ = viz.visualize_image_attr_multiple(attributions_mult[34,:,:, 0:1],\n",
    "                                          samp_all[34,:, :, 0:1],\n",
    "                                          [\"original_image\", \"heat_map\"],\n",
    "                                          [\"all\", \"all\"],\n",
    "                                          show_colorbar=True,\n",
    "                                            outlier_perc=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that dimensions are same for samples and attirbutions (w.r.t to bands)\n",
    "print(samp_all[k, :, :, i:i+1].shape, attributions_mult[k,:,:, i:i+1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributions_visualization (attributions, image, cmap=None, method=None):\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=2, squeeze=False, figsize=(16, 16))\n",
    "    axs[0, 0].set_title('Band 8 Image', fontsize =16)\n",
    "    _ = axs[0, 0].imshow(image[:, :, 0:1], vmin = 0, vmax = 1)\n",
    "    axs[0, 0].axis('off')\n",
    "    fig.colorbar(_, ax = axs[0, 0])\n",
    "\n",
    "    \n",
    "    axs[0, 1].set_title( method+' Band 8 Attributions', fontsize =16)\n",
    "    _ =axs[0, 1].imshow(_normalize_image_attr(attr=attributions[:,:, 0:1], sign= \"all\", outlier_perc= 2.0),\n",
    "                        cmap=plt.cm.PuOr, vmin = -1, vmax = 1)\n",
    "    axs[0, 1].axis('off')\n",
    "    fig.colorbar(_, ax = axs[0, 1])\n",
    "\n",
    "    \n",
    "    axs[1, 0].set_title('Band 9 Image', fontsize =16)\n",
    "    _ = axs[1, 0].imshow(image[:, :, 1:2], vmin = 0, vmax = 1)\n",
    "    axs[1, 0].axis('off')\n",
    "    fig.colorbar(_, ax = axs[1, 0])\n",
    "\n",
    "    axs[1, 1].set_title(method+' Band 9 Attributions', fontsize =16)\n",
    "    _ = axs[1, 1].imshow(_normalize_image_attr(attr=attributions[:,:, 1:2] , sign= \"all\", outlier_perc= 2.0),\n",
    "                         cmap=plt.cm.PuOr, vmin = -1, vmax = 1)\n",
    "    axs[1, 1].axis('off')\n",
    "    fig.colorbar(_, ax = axs[1, 1])\n",
    "\n",
    "    axs[2, 0].set_title('Band 10 Image', fontsize =16)\n",
    "    _ = axs[2, 0].imshow(image[:, :, 2:3], vmin = 0, vmax = 1)\n",
    "    axs[2, 0].axis('off')\n",
    "    fig.colorbar(_, ax = axs[2, 0])\n",
    "\n",
    "    axs[2, 1].set_title(method+' Band 10 Attributions', fontsize =16)\n",
    "    _ = axs[2, 1].imshow(_normalize_image_attr(attr=attributions[:,:, 2:3] , sign= \"all\", outlier_perc= 2.0),\n",
    "                         cmap=plt.cm.PuOr, vmin = -1, vmax = 1)\n",
    "    axs[2, 1].axis('off')\n",
    "    fig.colorbar(_, ax = axs[2, 1])\n",
    "    \n",
    "    axs[3, 0].set_title('Band 14 Image', fontsize =16)\n",
    "    _ = axs[3, 0].imshow(image[:, :, 3:4], vmin = 0, vmax = 1)\n",
    "    axs[3, 0].axis('off')\n",
    "    fig.colorbar(_, ax = axs[3, 0])\n",
    " \n",
    "    axs[3, 1].set_title(method+' Band 14 Attributions', fontsize =16)\n",
    "    _ = axs[3, 1].imshow(_normalize_image_attr(attr=attributions[:,:, 3:4] , sign= \"all\", outlier_perc= 2.0),\n",
    "                         cmap=plt.cm.PuOr, vmin = -1, vmax = 1)\n",
    "    axs[3, 1].axis('off')\n",
    "    fig.colorbar(_, ax = axs[3, 1])\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [27, 34, 81, 110]:\n",
    "    title = \"DL\" +str(i)\n",
    "    _ = attributions_visualization(attributions_mult[i], samp_all[i], method = \"Deeplift\")\n",
    "    _.savefig(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Older visualization method\n",
    "for k in ([27, 44, 81, 110]):\n",
    "    for i in range(4):\n",
    "        _ = viz.visualize_image_attr_multiple(attributions_mult[k,:,:, i:i+1],\n",
    "                                          samp_all[k,:, :, i:i+1],\n",
    "                                          [\"original_image\", \"heat_map\"],\n",
    "                                          [\"all\", \"all\"],\n",
    "                                          show_colorbar=True,\n",
    "                                            outlier_perc=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attributions Plotting for the four bands\n",
    "attributions_channels = attributions_mult.transpose(3, 1, 2, 0)\n",
    "fig, axs = plt.subplots(2, 2, figsize = (18,12), sharey = False)\n",
    "attributions_channels.shape\n",
    "l =[\"Upper-level Tropospheric Water Vapor (Band 8)\", \"Mid-level Tropospheric Water Vapor (Band 9)\",\n",
    "    \"Lower-level Tropospheric Water Vapor (Band 10)\", \"Infrared Longwave Window (Band 14)\"]\n",
    "it = 0\n",
    "histogram_store = []\n",
    "for i in range(2):\n",
    "    for j in range (2):\n",
    "        attributions_channels[it].flatten()\n",
    "        axs[i, j].set_title(\"Deeplift Attribution for \" + l[it], fontsize=16, pad=15 )\n",
    "        axs[i,j].set_xlabel(\"Attribution Values\", fontsize = 14)\n",
    "        axs[i,j].set_ylabel(\"Log Counts\", fontsize=14)\n",
    "        _ =axs[i, j].hist(attributions_channels[it].flatten(), bins = 200, density = True, range = (-0.2, 0.2), log=True)\n",
    "        histogram_store.append(_[0])\n",
    "        it += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"DL_attrbChanels\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do similar analysis using Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_gradients = IntegratedGradients(model)\n",
    "attributions_mult = []\n",
    "for idx, input1 in enumerate(all_images):\n",
    "    input1 = torch.tensor(input1)\n",
    "    input1 = input1.unsqueeze(0)\n",
    "    prediction_score, pred_label_idx = torch.topk(total_outputs[idx], 1)\n",
    "    attributions_ig = integrated_gradients.attribute(input1, target = pred_label_idx, n_steps= 50)\n",
    "    attributions_mult.append(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)))\n",
    "attributions_mult = np.array(attributions_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_all= np.transpose(all_images, (0, 2, 3, 1))\n",
    "plt.figure(figsize=(7.5,5))\n",
    "plt.xlabel(\"Pixel Values\", fontsize = 14)\n",
    "plt.ylabel(\"Integrated Gradient Attribution\", fontsize = 14)\n",
    "plt.title(\"Density Heatmap of Inputs and Attributions\", fontsize = 16, pad =15)\n",
    "hi_ = plt.hist2d(samp_all.flatten(), attributions_mult.flatten(), range =[[0, 1.0],[-0.6, 0.6]], density = True, bins= (50, 50), cmap = plt.cm.jet, norm = matplotlib.colors.LogNorm())\n",
    "plt.colorbar()\n",
    "plt.savefig(\"IG_InpvsAttrb\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = attr_classviz(attributions = attributions_mult, Range= [[0.0, 1.0], [-0.6, 0.6]], Share= False)\n",
    "_.savefig('IGact_class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = attr_classviz(attributions = attributions_mult, Range= [[0.0, 1.0], [-0.6, 0.6]], Share= False, predictions = predicted_dict, size = len(predicted_dict), Pred=True)\n",
    "_.savefig(\"IGpred_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in ([27, 44, 81, 110]):\n",
    "#     for i in range(4):\n",
    "#         _ = viz.visualize_image_attr_multiple(attributions_mult[k,:,:, i:i+1],\n",
    "#                                           samp_all[k,:, :, i:i+1],\n",
    "#                                           [\"original_image\", \"heat_map\"],\n",
    "#                                           [\"all\", \"all\"],\n",
    "#                                           show_colorbar=True,\n",
    "#                                             outlier_perc=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [27, 34, 81, 110]:\n",
    "    title = \"IG\" +str(i)\n",
    "    _ = attributions_visualization(attributions_mult[i], samp_all[i], method = \"Integrated Gradients\")\n",
    "    _.savefig(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_channels = attributions_mult.transpose(3, 1, 2, 0)\n",
    "fig, axs = plt.subplots(2, 2, figsize = (18,12), sharey = False)\n",
    "attributions_channels.shape\n",
    "l =[\"Upper-level Tropospheric Water Vapor (Band 8)\", \"Mid-level Tropospheric Water Vapor (Band 9)\",\n",
    "    \"Lower-level Tropospheric Water Vapor (Band 10)\", \"Infrared Longwave Window (Band 14)\"]\n",
    "it = 0\n",
    "histogram_store = []\n",
    "for i in range(2):\n",
    "    for j in range (2):\n",
    "        attributions_channels[it].flatten()\n",
    "        axs[i, j].set_title(\"Integrated Gradients Attribution for \" + l[it], fontsize=16, pad=10 )\n",
    "        axs[i,j].set_xlabel(\"Attribution Values\", fontsize = 14)\n",
    "        axs[i,j].set_ylabel(\"Log Counts\", fontsize=14)\n",
    "        _ =axs[i, j].hist(attributions_channels[it].flatten(), bins = 200, density = True, range = (-0.6, 0.6), log=True)\n",
    "        histogram_store.append(_[0])\n",
    "        it += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"IG_attrbChanels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat of previous analysis with the Baseline set to the average bin 0 image \n",
    "#### Implemented for Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_gradients = IntegratedGradients(model)\n",
    "attributionsBase_mult = []\n",
    "\n",
    "for input1 in all_images:\n",
    "    input1 = torch.tensor(input1)\n",
    "    input1 = input1.unsqueeze(0)\n",
    "    prediction_score, pred_label_idx = torch.topk(total_outputs[idx], 1)\n",
    "    attributionsBase_ig = integrated_gradients.attribute(input1, target = pred_label_idx, n_steps = 250, baselines= base_zero.unsqueeze(0) )\n",
    "    attributionsBase_mult.append(np.transpose(attributionsBase_ig.squeeze().cpu().detach().numpy(), (1,2,0)))\n",
    "attributionsBase_mult = np.array(attributionsBase_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Pixel Value\")\n",
    "plt.ylabel(\"Integrated Gradient Attribution\")\n",
    "plt.title(\"2D Histogram of Input Values vs. Attribution Values \")\n",
    "hi_ = plt.hist2d(samp_all.flatten(), attributionsBase_mult.flatten(), density = True, bins= (50, 50), cmap = plt.cm.jet, norm = matplotlib.colors.LogNorm())\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributionsBase_mult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize = (18,12), sharey = True)\n",
    "l =[\"Band 8 Attributions\", \"Band 9 Attributions\",\n",
    "    \"Band 10 Attributions\", \"Band 14 Attributions\"]\n",
    "\n",
    "it = 0\n",
    "for i in range(2):\n",
    "    for j in range (2):\n",
    "        axs[i, j].set_title(\"2D Histogram of Input Values vs. \" + l[it] )\n",
    "        axs[i,j].set_xlabel(\"Pixel Value\")\n",
    "        axs[i,j].set_ylabel(\"Integrated Gradient Attribution\")\n",
    "        hi_ = axs[i,j].hist2d(samp_all[:, :, :, it].flatten(), attributionsBase_mult[:, :, :, it].flatten(), density = True, bins= (50, 50), cmap = plt.cm.jet, norm = matplotlib.colors.LogNorm())\n",
    "        ax = axs[i,j]\n",
    "        fig.colorbar(hi_[3], ax = ax)\n",
    "        it += 1\n",
    "\n",
    "plt.subplots_adjust(left=0.2,\n",
    "                    bottom=0.2, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.2, \n",
    "                    hspace=0.3)\n",
    "# plt.savefig(\"bandwise2d.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize = (18,12), sharey = True)\n",
    "classifications = ['0 Bin', '1 Bin', '2 Bin', '3 Bin']\n",
    "\n",
    "it = 0\n",
    "for i in range(2):\n",
    "    for j in range (2):\n",
    "        axs[i, j].set_title(\"Input Values vs. Attributions for \" +\n",
    "                            classifications[it])\n",
    "        axs[i,j].set_xlabel(\"Pixel Value\")\n",
    "        axs[i,j].set_ylabel(\"Integrated Gradient Attributions\")\n",
    "        hi_ = axs[i,j].hist2d(samp_all[random_size*(it):random_size+random_size*(it), :, :, :].flatten(), \n",
    "                              attributionsBase_mult[random_size *(it):random_size+random_size*(it), :, :, :].flatten(),\n",
    "                              density = True, bins= (50, 50), cmap = plt.cm.jet, norm = matplotlib.colors.LogNorm())\n",
    "        ax = axs[i,j]\n",
    "        fig.colorbar(hi_[3], ax = ax)\n",
    "        it += 1\n",
    "\n",
    "plt.subplots_adjust(left=0.2,\n",
    "                    bottom=0.2, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.2, \n",
    "                    hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize = (18,12), sharey = True)\n",
    "classifications = ['0 Bin', '1 Bin', '2 Bin', '3 Bin']\n",
    "\n",
    "it = 0\n",
    "for i in range(2):\n",
    "    for j in range (2):\n",
    "        if(it == len(predicted_dict)):\n",
    "           break\n",
    "        axs[i, j].set_title(\"Input Values vs. Attributions for \" +\n",
    "                            classifications[it])\n",
    "        axs[i,j].set_xlabel(\"Pixel Value\")\n",
    "        axs[i,j].set_ylabel(\"Integrated Gradient Attributions\")\n",
    "        hi_ = axs[i,j].hist2d(samp_all[predicted_dict[it], :, :, :].flatten(), \n",
    "                              attributionsBase_mult[predicted_dict[it], :, :, :].flatten(),\n",
    "                              density = True, bins= (50, 50), cmap = plt.cm.jet, norm = matplotlib.colors.LogNorm())\n",
    "        ax = axs[i,j]\n",
    "        fig.colorbar(hi_[3], ax = ax)\n",
    "        it += 1\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "    \n",
    "\n",
    "plt.subplots_adjust(left=0.2,\n",
    "                    bottom=0.2, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.2, \n",
    "                    hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_channels = attributionsBase_mult.transpose(3, 1, 2, 0)\n",
    "fig, axs = plt.subplots(2, 2, figsize = (18,12), sharey = True)\n",
    "attributions_channels.shape\n",
    "l =[\"Upper-level Tropospheric Water Vapor (Band 8)\", \"Mid-level Tropospheric Water Vapor (Band 9)\",\n",
    "    \"Lower-level Tropospheric Water Vapor (Band 10)\", \"Infrared Longwave Window (Band 14)\"]\n",
    "it = 0\n",
    "histogram_store = []\n",
    "for i in range(2):\n",
    "    for j in range (2):\n",
    "        attributions_channels[it].flatten()\n",
    "        axs[i, j].set_title(\"Integrated Gradients for \" + l[it] )\n",
    "        axs[i,j].set_xlabel(\"Attribution Values\")\n",
    "        axs[i,j].set_ylabel(\"Counts\")\n",
    "        _ =axs[i, j].hist(attributions_channels[it].flatten(), bins = 557, density = True, log=True, range=(-0.6, 0.6))\n",
    "        histogram_store.append(_[0])\n",
    "        it += 1\n",
    "\n",
    "plt.subplots_adjust(left=0.2,\n",
    "                    bottom=0.2, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.2, \n",
    "                    hspace=0.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
