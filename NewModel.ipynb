{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm_base\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    if hasattr(tqdm_base, '_instances'):\n",
    "        for instance in list(tqdm_base._instances):\n",
    "            tqdm_base._decr_instances(instance)\n",
    "    return tqdm_base(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the device: cpu or gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to use device cuda:0\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")\n",
    "\n",
    "if is_cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f'Preparing to use device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/glade/u/home/gwallach/goes16ci/train_data_scaled.pkl','rb') as f:\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved shape: (151246, 32, 32, 4)\n",
      "Reshaped to: (151246, 4, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Saved shape:\", x.shape)\n",
    "x = x.transpose(0,3,1,2)\n",
    "print(\"Reshaped to:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/glade/u/home/gwallach/goes16ci/train_counts.pkl','rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved shape: (151246,)\n",
      "Reshaped to: (151246, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Saved shape:\", y.shape)\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "print(\"Reshaped to:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt0 = list(np.where(y >= 1)[0])\n",
    "# lt0 = list(np.where(y < 1)[0])\n",
    "# take_a_sample = random.sample(lt0, 10)\n",
    "\n",
    "# sample = gt0 + take_a_sample\n",
    "\n",
    "# x = x[sample]\n",
    "# y = y[sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train / test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state = 5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the image data is already scaled. only scale the lightning counts: subtract the mean and divide by sigma to get z-scores\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "#y_train = y_scaler.fit_transform(Y_train)\n",
    "#y_test = y_scaler.transform(Y_test)\n",
    "\n",
    "# y_train = np.log1p(Y_train)\n",
    "# y_test = np.log1p(Y_test)\n",
    "\n",
    "y_train = np.where(Y_train[:] > 0.0, 1, 0)\n",
    "y_test = np.where(Y_test[:] > 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(np.log1p(np.log1p(Y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv3x3(in_planes, out_planes, stride=1, dilation=1):\n",
    "#     \"3x3 convolution with padding\"\n",
    "#     # here with dilation\n",
    "#     return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1+(dilation-1)*(3-1), dilation=dilation, bias=False)\n",
    "\n",
    "\n",
    "# class BasicBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "\n",
    "#     def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n",
    "#         super(BasicBlock, self).__init__()\n",
    "#         self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "#         self.bn1 = nn.BatchNorm2d(planes)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = conv3x3(planes, planes)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "#         self.downsample = downsample\n",
    "#         self.stride = stride\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         residual = x\n",
    "\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "\n",
    "#         if self.downsample is not None:\n",
    "#             residual = self.downsample(x)\n",
    "\n",
    "#         out += residual\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         return out\n",
    "\n",
    "    \n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, block, layers, fcl_layers = [1], dr = 0.0, output_size = 1):\n",
    "#         self.inplanes = 64\n",
    "        \n",
    "#         super(Net, self).__init__()\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3,\n",
    "#                                bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "#         # note the increasing dilation\n",
    "#         self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "#         self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilation=1)\n",
    "#         self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n",
    "#         self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n",
    "\n",
    "#         # these layers will not be used\n",
    "#         self.avgpool = nn.AvgPool2d(4)\n",
    "#         #self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "#                 m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.weight.data.fill_(1)\n",
    "#                 m.bias.data.zero_()\n",
    "                \n",
    "#         ### Set up the fully-connected network to predict the lightning count\n",
    "#         conv_flatten = 512 * block.expansion\n",
    "#         if len(fcl_layers) > 0:\n",
    "#             fcn = [\n",
    "#                 nn.Linear(conv_flatten, fcl_layers[0]),\n",
    "#                 nn.BatchNorm1d(fcl_layers[0]),\n",
    "#                 nn.LeakyReLU(),\n",
    "#                 nn.Dropout(dr)\n",
    "#             ]\n",
    "#             if len(fcl_layers) == 1:\n",
    "#                 fcn.append(nn.Linear(fcl_layers[0], output_size))\n",
    "#             else:\n",
    "#                 for i in range(len(fcl_layers)-1):\n",
    "#                     fcn += [\n",
    "#                         nn.Linear(fcl_layers[i], fcl_layers[i+1]),\n",
    "#                         nn.BatchNorm1d(fcl_layers[i+1]),\n",
    "#                         nn.LeakyReLU(),\n",
    "#                         nn.Dropout(dr)\n",
    "#                     ]\n",
    "#                 fcn.append(nn.Linear(fcl_layers[i+1], output_size))\n",
    "#         else:\n",
    "#             fcn = [\n",
    "#                 nn.Linear(conv_flatten, output_size)\n",
    "#             ]\n",
    "#         if output_size > 1:\n",
    "#             fcn.append(torch.nn.LogSoftmax())\n",
    "#         self.fcn = nn.Sequential(*fcn)\n",
    "\n",
    "#     def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
    "#         downsample = None\n",
    "#         if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "#             downsample = nn.Sequential(\n",
    "#                 nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "#                           kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(planes * block.expansion),\n",
    "#             )\n",
    "\n",
    "#         layers = []\n",
    "#         layers.append(block(self.inplanes, planes, stride, 1, downsample))\n",
    "#         self.inplanes = planes * block.expansion\n",
    "#         for i in range(1, blocks):\n",
    "#             # here with dilation\n",
    "#             layers.append(block(self.inplanes, planes, dilation=dilation))\n",
    "\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "\n",
    "#         # deactivated layers\n",
    "#         x = self.avgpool(x)\n",
    "#         #print(x.shape)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fcn(x)\n",
    "\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, fcl_layers = [], dr = 0.0, output_size = 1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        #self.linear = nn.Linear(512*block.expansion, output_size)\n",
    "        self.fcn = self.make_fcn(512*block.expansion, output_size, fcl_layers, dr)\n",
    "        \n",
    "    def make_fcn(self, input_size, output_size, fcl_layers, dr):\n",
    "        if len(fcl_layers) > 0:\n",
    "            fcn = [\n",
    "                nn.Dropout(dr),\n",
    "                nn.Linear(input_size, fcl_layers[0]),\n",
    "                nn.BatchNorm1d(fcl_layers[0]),\n",
    "                torch.nn.LeakyReLU()\n",
    "            ]\n",
    "            if len(fcl_layers) == 1:\n",
    "                fcn.append(nn.Linear(fcl_layers[0], output_size))\n",
    "            else:\n",
    "                for i in range(len(fcl_layers)-1):\n",
    "                    fcn += [\n",
    "                        nn.Linear(fcl_layers[i], fcl_layers[i+1]),\n",
    "                        nn.BatchNorm1d(fcl_layers[i+1]),\n",
    "                        torch.nn.LeakyReLU(),\n",
    "                        nn.Dropout(dr)\n",
    "                    ]\n",
    "                fcn.append(nn.Linear(fcl_layers[i+1], output_size))\n",
    "        else:\n",
    "            fcn = [\n",
    "                nn.Dropout(dr),\n",
    "                nn.Linear(input_size, output_size)\n",
    "            ]\n",
    "        if output_size > 1:\n",
    "            fcn.append(torch.nn.LogSoftmax(dim=1))\n",
    "        return nn.Sequential(*fcn)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        x = self.fcn(out)\n",
    "        return x\n",
    "\n",
    "def ResNet18(fcl_layers = [], dr = 0.0, output_size = 1):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], fcl_layers = fcl_layers, dr = dr, output_size = output_size)\n",
    "\n",
    "def ResNet34(fcl_layers = [], dr = 0.0, output_size = 1):\n",
    "    return ResNet(BasicBlock, [3,4,6,3], fcl_layers = fcl_layers, dr = dr, output_size = output_size)\n",
    "\n",
    "def ResNet50(fcl_layers = [], dr = 0.0, output_size = 1):\n",
    "    return ResNet(Bottleneck, [3,4,6,3], fcl_layers = fcl_layers, dr = dr, output_size = output_size)\n",
    "\n",
    "def ResNet101(fcl_layers = [], dr = 0.0, output_size = 1):\n",
    "    return ResNet(Bottleneck, [3,4,23,3], fcl_layers = fcl_layers, dr = dr, output_size = output_size)\n",
    "\n",
    "def ResNet152(fcl_layers = [], dr = 0.0, output_size = 1):\n",
    "    return ResNet(Bottleneck, [3,8,36,3], fcl_layers = fcl_layers, dr = dr, output_size = output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 2\n",
    "fcl_layers = [1000]\n",
    "dropout = 0.5\n",
    "\n",
    "#model = Net(BasicBlock, [2,2,2,2], fcl_layers, dr = dropout, output_size = output_size).to(device)\n",
    "model = ResNet18(fcl_layers, dr = dropout, output_size = output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (fcn): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=1000, bias=True)\n",
       "    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=1000, out_features=2, bias=True)\n",
       "    (5): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model to make sure the architecture is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "X = torch.from_numpy(X_train[:2]).float().to(device)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4283, 0.5717],\n",
       "        [0.5403, 0.4597]], device='cuda:0', grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X).exp() # exp to turn the logits into probabilities, since we used LogSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_criterion = torch.nn.CrossEntropyLoss() # this is mean-squared error\n",
    "test_criterion = torch.nn.CrossEntropyLoss()  # this is mean absolute error\n",
    "\n",
    "# train_criterion = torch.nn.MSELoss() # this is mean-squared error\n",
    "# test_criterion = torch.nn.L1Loss()  # this is mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will help lower the learning rate when the model stops improving\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    patience = 1, \n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000 \n",
    "train_batch_size = 32\n",
    "valid_batch_size = 128\n",
    "batches_per_epoch = 1000\n",
    "\n",
    "patience = 10 # this is how many epochs we will keep training since we last saw a \"best\" model -- \"early stopping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 0.1845 train_acc: 0.9271 lr: 0.000100000000: 100%|██████████| 1000/1000 [04:13<00:00,  3.94it/s]\n",
      "Epoch 0 test_loss: 0.1879 test_acc: 0.9285: 100%|██████████| 236/236 [00:06<00:00, 35.64it/s]\n",
      "Epoch 1 train_loss: 0.1409 train_acc: 0.9459 lr: 0.000100000000: 100%|██████████| 236/236 [04:03<00:00,  1.03s/it]\n",
      "Epoch 1 test_loss: 0.1418 test_acc: 0.9466: 100%|██████████| 236/236 [00:06<00:00, 35.82it/s]\n",
      "Epoch 2 train_loss: 0.1315 train_acc: 0.9505 lr: 0.000100000000: 100%|██████████| 236/236 [03:59<00:00,  1.01s/it]\n",
      "Epoch 2 test_loss: 0.1513 test_acc: 0.9388: 100%|██████████| 236/236 [00:06<00:00, 34.33it/s]\n",
      "Epoch 3 train_loss: 0.1265 train_acc: 0.9523 lr: 0.000100000000: 100%|██████████| 236/236 [03:59<00:00,  1.02s/it]\n",
      "Epoch 3 test_loss: 0.1361 test_acc: 0.9479: 100%|██████████| 236/236 [00:06<00:00, 36.26it/s]\n",
      "Epoch 4 train_loss: 0.1188 train_acc: 0.9558 lr: 0.000100000000: 100%|██████████| 236/236 [03:55<00:00,  1.00it/s]\n",
      "Epoch 4 test_loss: 0.1552 test_acc: 0.9442: 100%|██████████| 236/236 [00:06<00:00, 36.56it/s]\n",
      "Epoch 5 train_loss: 0.1102 train_acc: 0.9592 lr: 0.000100000000: 100%|██████████| 236/236 [03:57<00:00,  1.01s/it]\n",
      "Epoch 5 test_loss: 0.1489 test_acc: 0.9484: 100%|██████████| 236/236 [00:06<00:00, 35.26it/s]\n",
      "Epoch 6 train_loss: 0.0740 train_acc: 0.9741 lr: 0.000010000000: 100%|██████████| 236/236 [04:07<00:00,  1.05s/it]\n",
      "Epoch 6 test_loss: 0.1610 test_acc: 0.9479: 100%|██████████| 236/236 [00:06<00:00, 36.87it/s]\n",
      "Epoch 7 train_loss: 0.0540 train_acc: 0.9818 lr: 0.000010000000: 100%|██████████| 236/236 [03:53<00:00,  1.01it/s]\n",
      "Epoch 7 test_loss: 0.1870 test_acc: 0.9455: 100%|██████████| 236/236 [00:06<00:00, 36.72it/s]\n",
      "Epoch 8 train_loss: 0.0392 train_acc: 0.9879 lr: 0.000001000000: 100%|██████████| 236/236 [03:51<00:00,  1.02it/s]\n",
      "Epoch 8 test_loss: 0.1951 test_acc: 0.9452: 100%|██████████| 236/236 [00:06<00:00, 36.56it/s]\n",
      "Epoch 9 train_loss: 0.0354 train_acc: 0.9892 lr: 0.000001000000: 100%|██████████| 236/236 [03:56<00:00,  1.00s/it]\n",
      "Epoch 9 test_loss: 0.2065 test_acc: 0.9438: 100%|██████████| 236/236 [00:06<00:00, 34.57it/s]\n",
      "Epoch 10 train_loss: 0.0340 train_acc: 0.9894 lr: 0.000000100000: 100%|██████████| 236/236 [03:56<00:00,  1.00s/it]\n",
      "Epoch 10 test_loss: 0.2062 test_acc: 0.9441: 100%|██████████| 236/236 [00:06<00:00, 36.35it/s]\n",
      "Epoch 11 train_loss: 0.0336 train_acc: 0.9899 lr: 0.000000100000: 100%|██████████| 236/236 [03:53<00:00,  1.01it/s]\n",
      "Epoch 11 test_loss: 0.2074 test_acc: 0.9440: 100%|██████████| 236/236 [00:06<00:00, 36.76it/s]\n",
      "Epoch 12 train_loss: 0.0336 train_acc: 0.9900 lr: 0.000000010000: 100%|██████████| 236/236 [03:50<00:00,  1.02it/s]\n",
      "Epoch 12 test_loss: 0.2084 test_acc: 0.9443: 100%|██████████| 236/236 [00:06<00:00, 37.10it/s]\n",
      "Epoch 13 train_loss: 0.0335 train_acc: 0.9898 lr: 0.000000010000: 100%|██████████| 236/236 [03:50<00:00,  1.02it/s]\n",
      "Epoch 13 test_loss: 0.2080 test_acc: 0.9441: 100%|██████████| 236/236 [00:06<00:00, 37.06it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch_test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    ### Train the model \n",
    "    model.train()\n",
    "\n",
    "    # Shuffle the data first\n",
    "    batch_loss = []\n",
    "    accuracy = []\n",
    "    indices = list(range(X_train.shape[0]))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # Now split into batches\n",
    "    train_batches_per_epoch = int(X_train.shape[0] / train_batch_size) \n",
    "    train_batches_per_epoch = min(batches_per_epoch, train_batches_per_epoch)\n",
    "    X = np.array_split(X_train[indices], train_batches_per_epoch)\n",
    "    Y = np.array_split(y_train[indices], train_batches_per_epoch)\n",
    "    \n",
    "    # custom tqdm so we can see the progress\n",
    "    training_data = zip(X, Y)\n",
    "    batch_group_generator = tqdm(\n",
    "        enumerate(training_data), \n",
    "        total=batches_per_epoch, \n",
    "        leave=True\n",
    "    )\n",
    "\n",
    "    for k, (x, y) in batch_group_generator:\n",
    "\n",
    "        # Converting to torch tensors and moving to GPU\n",
    "        inputs = torch.from_numpy(x).float().to(device)\n",
    "        lightning_counts = torch.from_numpy(y).long().to(device)\n",
    "\n",
    "        # Clear gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        pred_lightning_counts = model(inputs)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = train_criterion(pred_lightning_counts, lightning_counts.squeeze(-1))\n",
    "        \n",
    "        # compute the accuracy\n",
    "        acc = (torch.argmax(pred_lightning_counts, 1) == lightning_counts.squeeze(-1)).float()\n",
    "        accuracy += list(acc.cpu().numpy())\n",
    "\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # update tqdm\n",
    "        to_print = \"Epoch {} train_loss: {:.4f}\".format(epoch, np.mean(batch_loss))\n",
    "        to_print += \" train_acc: {:.4f}\".format(np.mean(accuracy))\n",
    "        to_print += \" lr: {:.12f}\".format(optimizer.param_groups[0]['lr'])\n",
    "        batch_group_generator.set_description(to_print)\n",
    "        batch_group_generator.update()\n",
    "                                  \n",
    "        if k >= train_batches_per_epoch and k > 0:\n",
    "            break\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ### Test the model \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_loss = []\n",
    "        accuracy = []\n",
    "        # split test data into batches\n",
    "        batches_per_epoch = int(X_test.shape[0] / valid_batch_size)\n",
    "        X = np.array_split(X_test, batches_per_epoch)\n",
    "        Y = np.array_split(y_test, batches_per_epoch)\n",
    "        \n",
    "        # custom tqdm so we can see the progress\n",
    "        test_data = zip(X, Y)\n",
    "        batch_group_generator = tqdm(\n",
    "            test_data, \n",
    "            total=batches_per_epoch, \n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        for (x, y) in batch_group_generator:\n",
    "            # Converting to torch tensors and moving to GPU\n",
    "            inputs = torch.from_numpy(x).float().to(device)\n",
    "            lightning_counts = torch.from_numpy(y).long().to(device)\n",
    "            # get output from the model, given the inputs\n",
    "            pred_lightning_counts = model(inputs)\n",
    "            # get loss for the predicted output\n",
    "            loss = test_criterion(pred_lightning_counts, lightning_counts.squeeze(-1))\n",
    "            batch_loss.append(loss.item())\n",
    "            # compute the accuracy\n",
    "            acc = (torch.argmax(pred_lightning_counts, 1) == lightning_counts.squeeze(-1)).float()\n",
    "            accuracy += list(acc.cpu().numpy())\n",
    "            # update tqdm\n",
    "            to_print = \"Epoch {} test_loss: {:.4f}\".format(epoch, np.mean(batch_loss))\n",
    "            to_print += \" test_acc: {:.4f}\".format(np.mean(accuracy))\n",
    "            batch_group_generator.set_description(to_print)\n",
    "            batch_group_generator.update()\n",
    "\n",
    "    test_loss = np.mean(batch_loss)\n",
    "    epoch_test_losses.append(test_loss)\n",
    "    \n",
    "    # Lower the learning rate if we are not improving\n",
    "    lr_scheduler.step(test_loss)\n",
    "\n",
    "    # Save the model if its the best so far.\n",
    "    if test_loss == min(epoch_test_losses):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': test_loss\n",
    "        }\n",
    "        torch.save(state_dict, \"best.pt\")\n",
    "        \n",
    "    # Stop training if we have not improved after X epochs\n",
    "    best_epoch = [i for i,j in enumerate(epoch_test_losses) if j == min(epoch_test_losses)][-1]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= patience:\n",
    "        break\n",
    "        \n",
    "    #-l gpu_type = gpu100, v100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute some test metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\n",
    "    \"best.pt\",\n",
    "    map_location=lambda storage, loc: storage\n",
    ")\n",
    "best_epoch = checkpoint[\"epoch\"]\n",
    "#model = Net(filter_sizes, fcl_layers).to(device)\n",
    "model = ResNet18(fcl_layers, dr = dropout, output_size = output_size).to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict on the test dataset with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    batch_loss = []\n",
    "    accuracy = []\n",
    "    # split test data into batches\n",
    "    batches_per_epoch = int(X_test.shape[0] / valid_batch_size)\n",
    "    X = np.array_split(X_test, batches_per_epoch)\n",
    "    Y = np.array_split(y_test, batches_per_epoch)\n",
    "    \n",
    "    for (x, y) in zip(X, Y):\n",
    "        # Converting to torch tensors and moving to GPU\n",
    "        inputs = torch.from_numpy(x).float().to(device)\n",
    "        lightning_counts = torch.from_numpy(y).long().to(device)\n",
    "        # get output from the model, given the inputs\n",
    "        pred_lightning_counts = model(inputs)\n",
    "        # get loss for the predicted output\n",
    "        loss = test_criterion(pred_lightning_counts, lightning_counts.squeeze(-1))\n",
    "        batch_loss.append(loss.item())\n",
    "        # compute the accuracy\n",
    "        acc = (torch.argmax(pred_lightning_counts, 1) == lightning_counts.squeeze(-1)).float()\n",
    "        accuracy += list(acc.cpu().numpy())\n",
    "        \n",
    "        y_true.append(lightning_counts.squeeze(-1))\n",
    "        y_pred.append(torch.argmax(pred_lightning_counts, 1))\n",
    "\n",
    "y_true = torch.cat(y_true, axis = 0)\n",
    "y_pred = torch.cat(y_pred, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1361466792996152 0.9479339\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(batch_loss), np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_norm = np.expm1(y_true.cpu().numpy()) #y_scaler.inverse_transform(y_true.cpu().numpy())\n",
    "#y_pred_norm = np.expm1(y_pred.cpu().numpy()) #y_scaler.inverse_transform(y_pred.cpu().numpy())\n",
    "\n",
    "y_true = y_true.cpu().numpy()\n",
    "y_pred = y_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95372506 0.04627494]\n",
      " [0.0632764  0.9367236 ]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_true, y_pred, normalize = 'true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245653527467791"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plotter(p, t, X):\n",
    "#     plt.scatter(p, t, alpha = 0.005)\n",
    "#     plt.plot(range(X), range(X), c = 'k')\n",
    "\n",
    "#     r2 = metrics.r2_score(t, p)\n",
    "#     acc = []\n",
    "#     for x,y in zip(t, p):\n",
    "#         if x > 0:\n",
    "#             acc.append(100.0*abs(x-y)/abs(x))\n",
    "#         else:\n",
    "#             acc.append(100.0*abs(x-y) / 1.0)\n",
    "    \n",
    "#     plt.xlabel(\"Predicted\", fontsize = 14)\n",
    "#     plt.ylabel(\"True\", fontsize = 14)\n",
    "#     plt.axis('square')\n",
    "    \n",
    "#     acc = np.mean(acc)\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     plt.xlim([-1,X])\n",
    "#     plt.ylim([-1,X])\n",
    "\n",
    "#     print(r2, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotter(y_pred, y_true, X = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y_true, bins = range(500), density = True)\n",
    "# plt.xlim([-1, 20])\n",
    "# plt.ylim([0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y_pred, bins = range(500), density = True)\n",
    "# plt.xlim([-1, 20])\n",
    "# plt.ylim([0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
